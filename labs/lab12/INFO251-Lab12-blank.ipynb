{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXGNWbGL08nX"
      },
      "source": [
        "# Part 0: preliminaries\n",
        "\n",
        "1. course evaluations are out\n",
        "\n",
        "2. suggest review session topics on Ed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vZeJXukU2Cl",
        "outputId": "a0f780c0-b381-4920-9d78-3bccb7ad19d2"
      },
      "outputs": [],
      "source": [
        "%pip install plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0f0kjRcU_XO"
      },
      "source": [
        "# Part 1: interactive 3D plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k4kKVA5U41B"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kVV8oSs2VE4R",
        "outputId": "fdb5bf31-512f-4c4a-928a-871c2292ce31"
      },
      "outputs": [],
      "source": [
        "# generate synthetic data\n",
        "\n",
        "n = 100\n",
        "r = np.random.uniform(0, 5, size=n)\n",
        "t = np.random.uniform(0, 2*np.pi, size=n)\n",
        "y = (r < 2.5).astype(str)\n",
        "\n",
        "X = pd.DataFrame({\n",
        "    \"x1\": r * np.cos(t),\n",
        "    \"x2\": r * np.sin(t),\n",
        "    \"y\": y\n",
        "})\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "0T-adkSNrqwL",
        "outputId": "5c78b81f-e4fc-41ec-afb5-fdf75a2f2238"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    X,\n",
        "    x = \"x1\", y = \"x2\", color = \"y\"\n",
        ")\n",
        "fig.update_layout(yaxis_scaleanchor=\"x\") # force equal aspect ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ZT2jGB2jVlUM",
        "outputId": "839f2626-a7a6-4623-9180-30157a1575d7"
      },
      "outputs": [],
      "source": [
        "X[\"x3\"] = ... # your code here \n",
        "\n",
        "fig_3d = px.scatter_3d(\n",
        "    X,\n",
        "    x = \"x1\", y = \"x2\", z = \"x3\",\n",
        "    color = \"y\",\n",
        ")\n",
        "\n",
        "fig_3d.update_traces(marker=dict(size=5))\n",
        "fig_3d.update_layout(yaxis_scaleanchor=\"x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsOnrQkhXtEg"
      },
      "source": [
        "# Part 2: higher-level fine-tuning tools\n",
        "\n",
        "## increasing abstraction\n",
        "\n",
        "### PS4: implement gradient descent from scratch\n",
        "```python\n",
        "for _ in range(max_iter):\n",
        "    theta = theta - learning_rate * gradient(loss, theta)\n",
        "```\n",
        "\n",
        "### PS6: implement training steps from scratch\n",
        "```python\n",
        "model = MyCoolNeuralNetwork()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for (X, y) in training_data_batch:\n",
        "        y_hat = model(X)\n",
        "\n",
        "        loss = loss_function(y, y_hat)\n",
        "        loss.backwards()\n",
        "\n",
        "        optimizer.step()\n",
        "    \n",
        "    model.eval()\n",
        "    for (X, y) in validation_data_batch:\n",
        "        ...\n",
        "```\n",
        "\n",
        "### PS7: higher-level APIs akin to sklearn's `model.fit()`/`model.predict()`\n",
        "\n",
        "## options\n",
        "- for Transformer models, [`transformers`](https://huggingface.co/docs/transformers/en/main_classes/trainer) has a number of automated training tools (see [LLM fine-tuning guide](https://huggingface.co/learn/llm-course/en/chapter3/3))\n",
        "\n",
        "- for generic pytorch models, you can use tools like [pytorch lightning](https://lightning.ai/docs/pytorch/stable/)\n",
        "\n",
        "- for computer vision model definitions, you've already seen `torchvision`. other options include: [`timm`](https://timm.fast.ai/), which has a similar API to the `transformers` library we'll use below and in PS7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG3MJQYSXvnF",
        "outputId": "624f9fc8-6e58-4a67-cb13-39cb15539f62"
      },
      "outputs": [],
      "source": [
        "%pip install timm transformers datasets evaluate \"transformers[torch]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE8eFm-4XzLM"
      },
      "outputs": [],
      "source": [
        "# from torchvision import datasets, transforms\n",
        "import torchvision as tv\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def load_cifar10(data_path = \".\"):\n",
        "    \"\"\"\n",
        "    Helper code to clean the CIFAR 10 dataset, and remove the unnecessary classes.\n",
        "    \"\"\"\n",
        "    class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "\n",
        "    cifar10 = tv.datasets.CIFAR10(\n",
        "        data_path, train=True, download=True,\n",
        "        transform=tv.transforms.ToTensor\n",
        "    )\n",
        "\n",
        "    cifar10_val = tv.datasets.CIFAR10(\n",
        "        data_path, train=False, download=True,\n",
        "        transform=tv.transforms.ToTensor\n",
        "    )\n",
        "\n",
        "    return cifar10, cifar10_val\n",
        "\n",
        "cifar10, cifar10_val = load_cifar10()\n",
        "\n",
        "\n",
        "\n",
        "label_map = {1: 0, 9: 1}\n",
        "class_names = ['car', 'truck']\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in list(label_map.keys())]\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in list(label_map.keys())]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OR_fI_nEWau"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from transformers import TrainingArguments, Trainer, ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "\n",
        "# Load the CIFAR-10 dataset and filter for car and truck classes\n",
        "label_map = {1: 0, 9: 1}\n",
        "class_names = ['car', 'truck']\n",
        "\n",
        "cifar_subset = load_dataset(\"cifar10\", split=\"train\")\n",
        "cifar_subset_val = load_dataset(\"cifar10\", split=\"test\")\n",
        "\n",
        "# Load the pre-trained ViT model and image processor\n",
        "VIT_MODEL = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(VIT_MODEL)\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    VIT_MODEL,\n",
        "    num_labels=len(class_names), ignore_mismatched_sizes=True,\n",
        "    id2label={i: c for i, c in enumerate(class_names)},\n",
        "    label2id={c: i for i, c in enumerate(class_names)}\n",
        ")\n",
        "\n",
        "# Define the evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Preprocess function to convert images to format model expects\n",
        "def preprocess_function(examples):\n",
        "    inputs = image_processor(examples[\"img\"], return_tensors=\"pt\")\n",
        "    inputs[\"labels\"] = [label_map[label] for label in examples[\"label\"]]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing function to the datasets\n",
        "cifar_processed = cifar_subset\\\n",
        "    .filter(lambda _: _[\"label\"] in list(label_map.keys()))\\\n",
        "    .map(preprocess_function, batched=True, remove_columns=cifar_subset.column_names)\n",
        "\n",
        "cifar_processed_val = cifar_subset_val\\\n",
        "    .filter(lambda _: _[\"label\"] in list(label_map.keys()))\\\n",
        "    .map(preprocess_function, batched=True, remove_columns=cifar_subset.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOhoJ9AQEhF0"
      },
      "outputs": [],
      "source": [
        "# Define the training arguments\n",
        "hyperparameters = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=...,\n",
        "    per_device_eval_batch_size=...,\n",
        "    num_train_epochs=...,\n",
        "    learning_rate=...,\n",
        "    weight_decay=...,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Create the Trainer and train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=hyperparameters,\n",
        "    train_dataset=cifar_processed,\n",
        "    eval_dataset=cifar_processed_val,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
