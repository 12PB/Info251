{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78d6514",
   "metadata": {},
   "source": [
    "# Lab 5: Gradient Descent\n",
    "- **Author:** Suraj R. Nair ([suraj.nair@berkeley.edu](mailto:suraj.nair@berkeley.edu)) (Based on previous material by Emily Aiken)\n",
    "- **Date:** February 19, 2025\n",
    "- **Course:** INFO 251: Applied machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a9fc3",
   "metadata": {},
   "source": [
    "### Topics:\n",
    "\n",
    "1. Minimization: Gradient descent (live coding)\n",
    "2. Exercises / Experiments (Your turn!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000ee2a",
   "metadata": {},
   "source": [
    "### Learning Objectives:\n",
    "At the end of this lab, you will understand:\n",
    "- How to derive and code up partial derivatives for gradient descent\n",
    "- When gradient descent converges to local and global minima\n",
    "- How the number of iterations, stopping conditions, the learning rate, and random seeds impact the convergence of gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76121f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9a36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1eb8b",
   "metadata": {},
   "source": [
    "### 0. Define a function\n",
    "\n",
    "Let's use a function with two inputs, $f(x,y)=(3x + 4)^2+ (y - 3)^2 + 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7840fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x, y):\n",
    "    return (3*x + 4)**2 + (y-3)**2 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd14de",
   "metadata": {},
   "source": [
    "### 1. Visualize the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7581dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function\n",
    "a = np.arange(-5, 5, 0.05)\n",
    "b = np.arange(-5, 5, 0.05)\n",
    "\n",
    "X, Y = np.meshgrid(a, b)\n",
    "Z = f(X, Y)\n",
    "\n",
    "sns.reset_orig()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', linewidth=0, antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6df720",
   "metadata": {},
   "source": [
    "### 2. Derive the minimum \n",
    "\n",
    "\n",
    "**Question:** Calculate the *gradient* for the function: $\\nabla f(x, y) = (\\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x, y)}{\\partial y})$.\n",
    "\n",
    "**Question:** What is the minimum of this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc428fc",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "$\\frac{\\partial f(x,y)}{\\partial x} =  ?$\n",
    "\n",
    "$\\frac{\\partial f(x, y)}{\\partial y} = ?$\n",
    "\n",
    "The minimum of this function lies at ________. At these coordinates, f(x, y) = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144782c2",
   "metadata": {},
   "source": [
    "### 3. Finding the minimum using gradient descent\n",
    "\n",
    "Let's write a simple function to find the minimum of this function using gradient descent! In order to do this:\n",
    "- define functions to calculate the partial derivatives\n",
    "- fill out the gradient_descent function (scaffolding provided)\n",
    "- test your function using the following inputs:\n",
    "    - px = 5\n",
    "    - py = 0\n",
    "    - learning_rate = 0.001\n",
    "    - max_iter = 5000\n",
    "- visualize the function, and the optimization path (code provided below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d947b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(px, py, learning_rate, dx,dy, max_iter = 1000):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    -----------\n",
    "    px: x coordinate of the starting point\n",
    "    py: y coordinate of the starting point\n",
    "    learning_rate: R, controls the size of the update / step\n",
    "    max_iter: maximum number of iterations\n",
    "    dx: function to calculate the gradient wrt x\n",
    "    dy: function to calculate the gradient wrt y\n",
    "    \n",
    "    Outputs\n",
    "    xpath: x coordinates that are visited  \n",
    "    ypath: y coordinates that are visited\n",
    "    zpath: z coordinates that are visited\n",
    "    \"\"\"\n",
    "    \n",
    "    return xpath, ypath, zpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cb0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete:\n",
    "dx = \n",
    "dy = \n",
    "\n",
    "xpath, ypath, zpath = gradient_descent(5, 0, 0.001, dx, dy, max_iter = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6248fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function, and the optimization path\n",
    "a = np.arange(-5, 5, 0.01)\n",
    "b = np.arange(-5, 5, 0.01)\n",
    "\n",
    "X, Y = np.meshgrid(a, b)\n",
    "Z = f(X, Y)\n",
    "\n",
    "sns.reset_orig()\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', linewidth=0, antialiased=False)\n",
    "ax.plot(xpath, ypath, zpath, 'r.-', linewidth=2, markersize=10, label='Optimization Path', zorder = 10)\n",
    "\n",
    "# Mark start and end points\n",
    "ax.scatter(xpath[0], ypath[0], zpath[0], color='green', s=100, label='Start', zorder = 20)\n",
    "ax.scatter(xpath[-1], ypath[-1], zpath[-1], color='red', s=100, label='End', zorder = 10)\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdc3c6",
   "metadata": {},
   "source": [
    "### Your Turn!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c041ab42",
   "metadata": {},
   "source": [
    "**0. Number of iterations:** Plot the parameters (in this case, x and y) as a function of the number of iterations / epochs. What do you observe? (Code provided below, focus on interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
    "\n",
    "ax[0].plot(range(len(xpath)), xpath, color='firebrick', linewidth=3)\n",
    "ax[0].set_title('Convergence of X')\n",
    "ax[0].set_ylabel('Value of X')\n",
    "\n",
    "ax[1].plot(range(len(ypath)), ypath, color='darkorange', linewidth=3)\n",
    "ax[1].set_title('Convergence of Y')\n",
    "ax[1].set_ylabel('Value of Y')\n",
    "\n",
    "for a in range(len(ax)):\n",
    "    ax[a].set_xlabel('Iteration')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558b96d",
   "metadata": {},
   "source": [
    "**1. Stopping Condition:** Add a stopping condition: modify the *gradient_descent* function.  Instead of max_iter, implement a *stopping condition*, where we'll stop running gradient descent if *k* iterations in a row all have approximately the same value. \n",
    "- think carefully about how you would implement this stopping condition\n",
    "- complete the *gradient_descent_w_stopping*\n",
    "- test your function using the following inputs:\n",
    "    - px = 5\n",
    "    - py = 0\n",
    "    - learning_rate = 0.001\n",
    "    - compare how long it takes for convergence using stopping tolerance = [0.01, 0.001, 0.0001, ......]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d95a904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_w_stopping(px, py, learning_rate, dx, dy, stopping_tolerance):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    -----------\n",
    "    px: x coordinate of the starting point\n",
    "    py: y coordinate of the starting point\n",
    "    learning_rate: R, controls the size of the update / step\n",
    "    dx: function to calculate the gradient wrt x\n",
    "    dy: function to calculate the gradient wrt y\n",
    "    stopping_tolerance: tolerance\n",
    "    \n",
    "    Outputs\n",
    "    xpath: x coordinates that are visited  \n",
    "    ypath: y coordinates that are visited\n",
    "    zpath: z coordinates that are visited\n",
    "    num_iter: the number of iterations at the time of stopping\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return xpath, ypath, zpath, num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a18ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09c0732b",
   "metadata": {},
   "source": [
    "**2. Learning Rates:** Investigate the impact of different learning rates on the convergence of this gradient descent algorithm. \n",
    "- Fix a starting point (i.e x and y values): sticking to (5, 0) is fine, but feel free to chose your own.  \n",
    "- For each learning rate, run your gradient descent function for 1000 iterations. Iterate over the following set of learning rates: [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001]\n",
    "- Create a plot, where learning rates are on the x-axis, and the estimated values of *x* are on the y-axis. What do you observe?\n",
    "- In similar fashion, create a second plot, this time using the estimated values of *y*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb341027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fccbff2",
   "metadata": {},
   "source": [
    "**3. Random Initialization:** Confirm that your algorithm converges irrespective of the starting point. \n",
    "\n",
    "In order to do this:\n",
    "- Create 250 random initializations for x and y.\n",
    "- For each random initialization, run gradient descent, and store the outputs. (This might take a few seconds on your computer, so start by setting *max_iter* = 1000 for each random initialization). \n",
    "    - Note: If you are using the gradient descent function we defined earlier, you only need to store the final values in xpath / ypath for this exercise. \n",
    "- Plot the distribution of the obtained x and y values (separate graphs). In each graph, add vertical lines indicating the *mean* of x / y values respectively.  \n",
    "\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e3042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
