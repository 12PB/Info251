{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPM vs Logistic Regression\n",
    "\n",
    "### synthetic data \n",
    "\n",
    "Let's set up synthetic data representing students in the class. Assume ISchool students come to office hours more often. We want to predict the probability that a student comes to office hours, based on some observable characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set up test data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "n_students = 100\n",
    "n_features = 5\n",
    "\n",
    "p_ischool = 0.6\n",
    "p_other = 0.3\n",
    "\n",
    "odds_ischool = p_ischool/(1 - p_ischool)\n",
    "odds_other = p_other/(1 - p_other)\n",
    "\n",
    "log_odds_ischool = np.log(odds_ischool)\n",
    "log_odds_other = np.log(odds_other)\n",
    "\n",
    "\n",
    "print(\"ischool odds (3:2):\", odds_ischool, \"log-odds\", log_odds_ischool)\n",
    "print(\"non-ischool odds (3:7):\", odds_other, \"log-odds\", log_odds_other)\n",
    "\n",
    "np.random.seed(0)\n",
    "students = pd.DataFrame(np.random.normal(size = (n_students, n_features)), columns=[f\"feature_{i}\" for i in range(1, n_features + 1)])\n",
    "students[\"ischool\"] = (students.index > (n_students/2)).astype(int) # set the first half of the dataframe to have ischool = 0\n",
    "X = students.values\n",
    "y = np.random.normal(loc = p_other, scale = 0.1, size = (n_students,)) + students[\"ischool\"] * (p_ischool - p_other) + X.sum(axis = 1)/10\n",
    "y = np.clip(y, a_min = 0, a_max = 1)\n",
    "students[\"OH\"] = y\n",
    "\n",
    "plt.scatter(list(range(n_students//2)), y[:n_students//2], color = \"crimson\", label = \"department: other\")\n",
    "plt.scatter(list(range(n_students//2, n_students)), y[n_students//2:], color = \"dodgerblue\", label = \"department: ischool\")\n",
    "plt.legend(handlelength = 0.5, loc=\"upper center\", ncols = 2)\n",
    "plt.title(\"probability of attending office hours\\n\")\n",
    "plt.xlabel(\"student id\")\n",
    "plt.ylabel(\"P(OH)\")\n",
    "plt.hlines([0, 1], 0, 100, color=\"gray\", linestyle=\"dotted\")\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.xlim(-1, 101)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPM\n",
    "\n",
    "The LPM is just a linear regression of probability on the student characteristics.\n",
    "\n",
    "$$ y = X'\\beta + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fit the model and see how well we did\n",
    "X1 = sm.add_constant(X)\n",
    "lpm = sm.OLS(y, X1).fit()\n",
    "display(lpm.summary())\n",
    "\n",
    "# now let's predict \n",
    "y_hat_lpm = lpm.predict(X1)\n",
    "\n",
    "# zoinks\n",
    "display(y_hat_lpm)\n",
    "plt.scatter(list(range(n_students//2)), y_hat_lpm[:n_students//2], color = \"crimson\", label = \"department: other\")\n",
    "plt.scatter(list(range(n_students//2, n_students)), y_hat_lpm[n_students//2:], color = \"dodgerblue\", label = \"department: ischool\")\n",
    "plt.legend(handlelength = 0.5, loc=\"upper center\", ncols = 2)\n",
    "plt.title(\"predicted probability of attending office hours (LPM)\\n\")\n",
    "plt.xlabel(\"student id\")\n",
    "plt.ylabel(\"P(OH)\")\n",
    "plt.hlines([0, 1], 0, 100, color=\"gray\", linestyle=\"dotted\")\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.xlim(-1, 101)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2: Logistic Regression\n",
    "\n",
    "In the LPM, there was nothing constraining predicted probabilities from staying in the valid range $[0, 1]$. To ensure predictions output valid probabilities, we can use a function that maps from the reals to the range $[0, 1]$, like the logistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.linspace(-10, 10)\n",
    "m = 0.07\n",
    "b = 0.5\n",
    "Y = m*X_ + b\n",
    "\n",
    "L = 1/(1 + np.exp(-X_))\n",
    "\n",
    "plt.plot(X_, Y, color = \"orange\", label = \"linear\")\n",
    "plt.plot(X_, L, color = \"yellowgreen\", label = \"logit\")\n",
    "plt.legend()\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.hlines([0, 1], X_[0], X_[-1], color=\"gray\", linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression transforms the data to constrain outputs to valid values:\n",
    "\n",
    "$$ \\log\\left(\\frac{y}{1-y}\\right) \\sim  X'\\beta + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fit the model and see how well we did\n",
    "X1 = sm.add_constant(X)\n",
    "logreg = sm.Logit(y, X1).fit()\n",
    "display(logreg.summary())\n",
    "\n",
    "# now let's predict \n",
    "y_hat_logreg = logreg.predict(X1)\n",
    "\n",
    "print(y_hat_logreg)\n",
    "plt.scatter(list(range(n_students//2)), y_hat_logreg[:n_students//2], color = \"crimson\", label = \"department: other\")\n",
    "plt.scatter(list(range(n_students//2, n_students)), y_hat_logreg[n_students//2:], color = \"dodgerblue\", label = \"department: ischool\")\n",
    "plt.legend(handlelength = 0.5, loc=\"upper center\", ncols = 2)\n",
    "plt.title(\"predicted probability of attending office hours (logistic regression)\\n\")\n",
    "plt.xlabel(\"student id\")\n",
    "plt.ylabel(\"P(OH)\")\n",
    "plt.hlines([0, 1], 0, 100, color=\"gray\", linestyle=\"dotted\")\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.xlim(-1, 101)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "logreg.params[\"x1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intepreting the logistic regression coefficients:\n",
    "\n",
    "- start with the logistic function:\n",
    "$$ \\log\\left(\\frac{y}{1-y}\\right) = X'\\beta $$\n",
    "\n",
    "- exponentiate:\n",
    "$$ \\frac{y}{1-y} = \\exp\\left(X'\\beta\\right) $$\n",
    "\n",
    "- multiply by $(1-y)$\n",
    "$$ y = (1- y)\\exp\\left(X'\\beta\\right) $$\n",
    "\n",
    "- distribute \n",
    "$$ y = \\exp\\left(X'\\beta\\right) - y \\exp\\left(X'\\beta\\right) $$\n",
    "\n",
    "- collect move terms with $y$ to the left\n",
    "$$ y + y \\exp\\left(X'\\beta\\right) = \\exp\\left(X'\\beta\\right) $$\n",
    "\n",
    "- factor out\n",
    "$$ y(1 + \\exp\\left(X'\\beta\\right)) = \\exp\\left(X'\\beta\\right) $$\n",
    "\n",
    "- divide\n",
    "$$ y = \\frac{\\exp\\left(X'\\beta\\right)}{1 + \\exp\\left(X'\\beta\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just regress on ischool affiliation to work through calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_binary = sm.Logit(y, sm.add_constant(students[\"ischool\"])).fit()\n",
    "\n",
    "display(logreg_binary.summary())\n",
    "\n",
    "\n",
    "a = logreg_binary.params[\"const\"]\n",
    "b = logreg_binary.params[\"ischool\"]\n",
    "\n",
    "def logistic(z):\n",
    "    return np.exp(z)/(1 + np.exp(z))\n",
    "\n",
    "\n",
    "# calculate log odds\n",
    "print(\"estimated log odds for non-ischool\", a, f\"(actual: {log_odds_other})\")\n",
    "\n",
    "print(\"estimated log odds for ischool\", a+b, f\"(actual: {log_odds_ischool})\")\n",
    "\n",
    "# convert to probabilities\n",
    "# non-ischool probability\n",
    "z0 = a + b*0\n",
    "p0 = logistic(z0)\n",
    "print(\"logreg estimate of p(OH) (non-ischool): \", round(p0, 3), f\"(actual: {p_other})\")\n",
    "\n",
    "# ischool probability \n",
    "z1 = a + b*1\n",
    "p1 = logistic(z1)\n",
    "print(\"logreg estimate of p(OH) (ischool): \", round(p1, 3), f\"(actual: {p_ischool})\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## things to remember:\n",
    "\n",
    "1. LPMs can predict negative probability\n",
    "\n",
    "2. Squashing your data through a function like the logistic function helps constrain your model output to valid values\n",
    "\n",
    "3. Many functions map from $(-\\infty, \\infty) \\mapsto [0, 1]$. We like the logistic because:\n",
    "    \n",
    "    a) the odds-ratio is somewhat interpretable\n",
    "    \n",
    "    b) the derivative is easy to calculate (we'll see this again in deep learning!)\n",
    "\n",
    "4. Logistic regression coefficients interpreted as changes in log-odds (cf. [this guide to interpretation](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
